{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/site-packages (1.3.2)\n",
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.14.1-cp38-cp38-macosx_10_9_x86_64.whl (10.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.5 MB 1.3 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kaixue/Library/Python/3.8/lib/python/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Collecting patsy>=0.5.4\n",
      "  Downloading patsy-1.0.2-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[K     |████████████████████████████████| 233 kB 63.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting packaging>=21.3\n",
      "  Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Installing collected packages: patsy, packaging, statsmodels\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.4\n",
      "    Uninstalling packaging-20.4:\n",
      "      Successfully uninstalled packaging-20.4\n",
      "Successfully installed packaging-25.0 patsy-1.0.2 statsmodels-0.14.1\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.8/site-packages (0.14.1)\n",
      "Requirement already satisfied: numpy<2,>=1.18 in /usr/local/lib/python3.8/site-packages (from statsmodels) (1.24.4)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.8/site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.0 in /usr/local/lib/python3.8/site-packages (from statsmodels) (1.2.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /usr/local/lib/python3.8/site-packages (from statsmodels) (1.10.1)\n",
      "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.8/site-packages (from statsmodels) (1.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kaixue/Library/Python/3.8/lib/python/site-packages (from python-dateutil>=2.7.3->pandas!=2.1.0,>=1.0->statsmodels) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing packages...\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing packages...\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Lasso, LassoCV, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Successfully loaded '401k.csv'. Shape: (9915, 24)\n",
      "Dropped 0 rows with missing values.\n",
      "Final dataset shape: (9915, 17)\n"
     ]
    }
   ],
   "source": [
    "# LOAD AND PREPROCESS DATA\n",
    "# --------------------------\n",
    "print(\"Loading and preprocessing data...\")\n",
    "file_path = '401k.csv'\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded '{file_path}'. Shape: {data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{file_path}' not found.\")\n",
    "    print(\"Please make sure the dataset is in the same directory as this notebook/script.\")\n",
    "    data = None\n",
    "\n",
    "if data is not None:\n",
    "    # --- Feature Engineering & Variable Definition ---\n",
    "    \n",
    "    # Create log_inc\n",
    "    # CONCEPT: We create log_inc (log of income) for a few reasons:\n",
    "    # 1. It pulls in outliers. Income data is often highly skewed (a few very rich people).\n",
    "    # 2. A 1-unit change in log_inc can be interpreted as a ~1% change in income,\n",
    "    #    which is often more meaningful than a $1 change.\n",
    "    # We use np.maximum(1, ...) to avoid log(0) errors if any incomes are $0.\n",
    "    data['log_inc'] = np.log(np.maximum(1, data['inc']))\n",
    "    \n",
    "    # Define the key variables for our analysis\n",
    "    \n",
    "    # Y_VAR: The Outcome (Dependent) Variable\n",
    "    # What we are trying to predict or explain.\n",
    "    Y_VAR = 'tw'  # Total Wealth\n",
    "    \n",
    "    # T_VAR: The Treatment Variable\n",
    "    # This is the variable whose causal effect we want to measure.\n",
    "    T_VAR = 'p401' # Participation in 401(k)\n",
    "    \n",
    "    # Z_VAR: The Instrumental Variable\n",
    "    # A special variable used to \"fix\" selection bias in the 2SLS model.\n",
    "    Z_VAR = 'e401' # Eligibility for 401(k)\n",
    "    \n",
    "    # CONTROL_VARS: The Control Variables (Covariates)\n",
    "    # All the other variables we specified that could also affect wealth.\n",
    "    # We include these to \"control for\" their effects.\n",
    "    CONTROL_VARS = [\n",
    "        'log_inc', 'age', 'fsize', 'marr', 'twoearn', 'db', 'ira', 'pira', \n",
    "        'hown', 'male', 'educ', 'hmort', 'hequity', 'hval'\n",
    "    ]\n",
    "    \n",
    "    # All variables needed for the analysis\n",
    "    ALL_VARS = [Y_VAR, T_VAR, Z_VAR] + CONTROL_VARS\n",
    "    \n",
    "    # Data Cleaning: Drop rows with missing values\n",
    "    # We only keep rows that have complete data for all variables we will use.\n",
    "    original_rows = data.shape[0]\n",
    "    data = data[ALL_VARS].dropna()\n",
    "    print(f\"Dropped {original_rows - data.shape[0]} rows with missing values.\")\n",
    "    print(f\"Final dataset shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading and preprocessing complete.\n",
      "Target (y): tw\n",
      "Treatment (t): p401\n",
      "Instrument (Z): e401\n",
      "Controls (X): ['log_inc', 'age', 'fsize', 'marr', 'twoearn', 'db', 'ira', 'pira', 'hown', 'male', 'educ', 'hmort', 'hequity', 'hval']\n",
      "Features for Naive LASSO: ['p401', 'log_inc', 'age', 'fsize', 'marr', 'twoearn', 'db', 'ira', 'pira', 'hown', 'male', 'educ', 'hmort', 'hequity', 'hval']\n"
     ]
    }
   ],
   "source": [
    "# DEFINE FINAL VARIABLES FOR MODELS\n",
    "# -----------------------------------\n",
    "if 'data' in locals() and data is not None:\n",
    "    # y: The outcome variable (a Series)\n",
    "    y = data[Y_VAR]\n",
    "    \n",
    "    # t: The treatment variable (a Series)\n",
    "    t = data[T_VAR]\n",
    "    \n",
    "    # X: The control variables (a DataFrame)\n",
    "    # Note: This *excludes* the treatment 'p401'.\n",
    "    # This is used for the causal models (DML and 2SLS).\n",
    "    X = data[CONTROL_VARS]\n",
    "    \n",
    "    # Z: The instrumental variable (a Series)\n",
    "    Z = data[Z_VAR]\n",
    "    \n",
    "    # X_with_treatment: All features for the *predictive* model\n",
    "    # This *includes* the treatment 'p401' as a feature.\n",
    "    # This is used for the \"Naive LASSO\" in Part 1.\n",
    "    X_with_treatment = data[[T_VAR] + CONTROL_VARS]\n",
    "\n",
    "    print(\"Data loading and preprocessing complete.\")\n",
    "    print(f\"Target (y): {Y_VAR}\")\n",
    "    print(f\"Treatment (t): {T_VAR}\")\n",
    "    print(f\"Instrument (Z): {Z_VAR}\")\n",
    "    print(f\"Controls (X): {X.columns.tolist()}\")\n",
    "    print(f\"Features for Naive LASSO: {X_with_treatment.columns.tolist()}\")\n",
    "else:\n",
    "    print(\"Data was not loaded. Fix the previous cell and re-run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Naive LASSO Model (tw ~ p401 + controls)\n",
    "We chose this as a standard machine learning model for **prediction**.\n",
    "\n",
    "**Conceptual Clarification:** We call this \"Naive\" because it treats `p401` as just another feature. \n",
    "It doesn't know about \"selection bias.\" The coefficient it finds for `p401` is **biased** and \n",
    "represents *correlation*, not *causation*. It answers: \"How does `tw` change when `p401` is 1, \n",
    "*holding all else constant*?\" This is biased because people who choose `p401=1` are different \n",
    "in unobserved ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part 1: Naive LASSO Model (tw ~ p401 + controls) ---\n",
      "Best alpha selected by LassoCV: 79.2591\n",
      "Test Set RMSE: 66060.99\n",
      "Test Set R-squared: 0.63\n",
      "\n",
      "LASSO Coefficients:\n",
      "hequity    52505.221108\n",
      "ira        26943.488541\n",
      "hval       16121.140435\n",
      "log_inc    10949.202509\n",
      "age         7137.367674\n",
      "p401        4650.277035\n",
      "marr        3687.592929\n",
      "pira        3233.435161\n",
      "educ        1501.808628\n",
      "male         918.589618\n",
      "hmort          0.000000\n",
      "db         -1098.004872\n",
      "fsize      -1635.398022\n",
      "hown       -5883.377366\n",
      "twoearn    -8592.614474\n",
      "dtype: float64\n",
      "\n",
      "------------------------------------------------------\n",
      "Project Insight (Model Performance & Causal Effect):\n",
      "The coefficient for 'p401' in this *predictive* model is: 4650.28\n",
      "This is the 'Naive' (or 'Biased') estimate. It's likely high\n",
      "because it's capturing both the true effect of 401(k)s AND\n",
      "the fact that people who save a lot are more likely to sign up.\n",
      "This is **selection bias**.\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PART 1: NAIVE LASSO MODEL\n",
    "# -------------------------\n",
    "print(\"\\n--- Part 1: Naive LASSO Model (tw ~ p401 + controls) ---\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# We train the model on 80% of the data and test its performance on 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_with_treatment, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a 'Pipeline'\n",
    "# Our pipeline will:\n",
    "# 1. Impute: Fill any remaining missing values (as a safety step).\n",
    "# 2. Scale: Standardize all features.\n",
    "# 3. Lasso: Run the LASSO regression model.\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    \n",
    "    # CONCEPT: StandardScaler is CRITICAL for LASSO.\n",
    "    # LASSO penalizes large coefficients. If features are on different scales\n",
    "    # (e.g., age 25-64 vs hval 0-300000+), the model will unfairly\n",
    "    # penalize the coefficients for 'hval'. Scaling puts all features\n",
    "    # on a similar scale (mean 0, std dev 1).\n",
    "    ('scaler', StandardScaler()),\n",
    "    \n",
    "    # CONCEPT: LassoCV (Cross-Validation)\n",
    "    # This automatically tests many different 'alphas' (penalization strengths)\n",
    "    # using 10-fold cross-validation (cv=10) and selects the best one.\n",
    "    ('lasso', LassoCV(cv=10, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Get the best alpha (penalty strength) found by CV\n",
    "best_alpha = pipeline.named_steps['lasso'].alpha_\n",
    "print(f\"Best alpha selected by LassoCV: {best_alpha:.4f}\")\n",
    "\n",
    "# Evaluate on the unseen test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Set RMSE: {rmse:.2f}\")\n",
    "print(f\"Test Set R-squared: {r2:.2f}\")\n",
    "\n",
    "# Get coefficients\n",
    "lasso_model = pipeline.named_steps['lasso']\n",
    "lasso_coefs = pd.Series(lasso_model.coef_, index=X_with_treatment.columns)\n",
    "\n",
    "print(\"\\nLASSO Coefficients:\")\n",
    "print(lasso_coefs.sort_values(ascending=False))\n",
    "\n",
    "p401_coef_lasso = lasso_coefs.get('p401')\n",
    "print(\"\\n------------------------------------------------------\")\n",
    "print(\"Project Insight (Model Performance & Causal Effect):\")\n",
    "print(f\"The coefficient for 'p401' in this *predictive* model is: {p401_coef_lasso:.2f}\")\n",
    "print(\"This is the 'Naive' (or 'Biased') estimate. It's likely high\")\n",
    "print(\"because it's capturing both the true effect of 401(k)s AND\")\n",
    "print(\"the fact that people who save a lot are more likely to sign up.\")\n",
    "print(\"This is **selection bias**.\")\n",
    "print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Double-Selection (DML) LASSO (For Causal Effect)\n",
    "\n",
    "This is a modern econometric method using ML to estimate a **causal effect**.\n",
    "\n",
    "**Conceptual Clarification:** DML (from Chernozhukov, et al.) \"de-biases\" the `p401` coefficient. \n",
    "It recognizes that the bias in Part 1 comes from omitting variables that are related to *both* \n",
    "participation (`p401`) and wealth (`tw`).\n",
    "\n",
    "It works in 3 main steps (Steps 1, 2, 3 here are 2 stages in the paper):\n",
    "1.  **Stage 1 (Outcome):** Use LASSO to find all controls `X` that are good at predicting the \n",
    "outcome `tw`.\n",
    "2.  **Stage 2 (Treatment):** Use LASSO to find all controls `X` that are good at predicting \n",
    "the treatment `p401`. (This is the \"selection\" model).\n",
    "3.  **Final OLS:** Run a simple OLS regression of `tw ~ p401 + union_of_controls`, where \n",
    "union_of_controls` is the *combination* of all variables selected in Step 1 *and* Step 2.\n",
    "\n",
    "This ensures we control for *everything* that could be causing bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTION FOR DML\n",
    "# -----------------------\n",
    "def run_lasso_selection(X, y, alpha=None):\n",
    "    \"\"\"Helper function to run LASSO for feature selection.\"\"\"\n",
    "    if alpha is None:\n",
    "        # Use LassoCV to find the best alpha if one isn't provided\n",
    "        model = LassoCV(cv=10, random_state=42, n_jobs=-1).fit(X, y)\n",
    "    else:\n",
    "        model = Lasso(alpha=alpha, random_state=42).fit(X, y)\n",
    "    \n",
    "    # Get coefficients and return the *indices* of non-zero ones\n",
    "    coefs = np.abs(model.coef_)\n",
    "    print(coefs)\n",
    "    return model, np.where(coefs > 1e-6)[0] # 1e-6 to avoid tiny floating points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part 2: Double-Selection LASSO (For Causal Effect) ---\n",
      "[11814.86388836  6574.98674172  2111.34673033  3546.56256922\n",
      "  8221.51099888   486.8124728  26544.62497005  3770.61983464\n",
      "  5691.04689871  1442.96064232  1292.88114301     0.\n",
      " 53610.0127981  15351.66119028]\n",
      "Controls selected by outcome model (tw ~ X): ['log_inc', 'age', 'fsize', 'marr', 'twoearn', 'db', 'ira', 'pira', 'hown', 'male', 'educ', 'hequity', 'hval']\n",
      "Controls selected by treatment model (p401 ~ X): ['log_inc', 'age', 'fsize', 'marr', 'twoearn', 'db', 'pira', 'hown', 'male', 'educ', 'hmort', 'hequity']\n",
      "Union of selected controls: ['ira', 'db', 'hown', 'age', 'hval', 'hequity', 'hmort', 'marr', 'fsize', 'pira', 'educ', 'twoearn', 'male', 'log_inc']\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     tw   R-squared:                       0.607\n",
      "Model:                            OLS   Adj. R-squared:                  0.606\n",
      "Method:                 Least Squares   F-statistic:                     1091.\n",
      "Date:                Sun, 30 Nov 2025   Prob (F-statistic):               0.00\n",
      "Time:                        16:58:20   Log-Likelihood:            -1.2467e+05\n",
      "No. Observations:                9915   AIC:                         2.494e+05\n",
      "Df Residuals:                    9900   BIC:                         2.495e+05\n",
      "Df Model:                          14                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "p401        1.242e+04   1699.988      7.304      0.000    9084.821    1.57e+04\n",
      "ira            2.7402      0.098     28.026      0.000       2.549       2.932\n",
      "db         -2642.6914   1694.904     -1.559      0.119   -5965.049     679.666\n",
      "hown       -1.319e+04   2519.537     -5.236      0.000   -1.81e+04   -8254.129\n",
      "age          658.6833     78.966      8.341      0.000     503.895     813.472\n",
      "hval        3.728e+11   1.21e+12      0.308      0.758      -2e+12    2.75e+12\n",
      "hequity    -3.728e+11   1.21e+12     -0.308      0.758   -2.75e+12       2e+12\n",
      "hmort      -3.728e+11   1.21e+12     -0.308      0.758   -2.75e+12       2e+12\n",
      "marr        8033.3590   2296.246      3.498      0.000    3532.250    1.25e+04\n",
      "fsize      -1417.4206    575.659     -2.462      0.014   -2545.830    -289.011\n",
      "pira        7978.8282   2212.285      3.607      0.000    3642.300    1.23e+04\n",
      "educ         476.5816    287.508      1.658      0.097     -86.993    1040.156\n",
      "twoearn    -1.775e+04   1980.910     -8.962      0.000   -2.16e+04   -1.39e+04\n",
      "male        3951.6772   1904.113      2.075      0.038     219.227    7684.127\n",
      "log_inc     1.446e+04   1280.548     11.295      0.000     1.2e+04     1.7e+04\n",
      "const       -1.67e+05   1.26e+04    -13.223      0.000   -1.92e+05   -1.42e+05\n",
      "==============================================================================\n",
      "Omnibus:                    15156.848   Durbin-Watson:                   1.972\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         10395075.483\n",
      "Skew:                           9.459   Prob(JB):                         0.00\n",
      "Kurtosis:                     160.494   Cond. No.                     3.57e+14\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.11e-15. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "------------------------------------------------------\n",
      "Project Insight (DML Causal Effect):\n",
      "The DML estimate for 'p401' is: 12417.14 (SE: 1699.99)\n",
      "This is the 'unbiased' ML-based estimate. Compare this to the\n",
      "Naive LASSO coefficient. This is a more credible causal effect.\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PART 2: DOUBLE-SELECTION LASSO\n",
    "# ------------------------------\n",
    "print(\"\\n--- Part 2: Double-Selection LASSO (For Causal Effect) ---\")\n",
    "\n",
    "# Standardize data for LASSO selection steps\n",
    "# As before, scaling is critical for LASSO\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=CONTROL_VARS)\n",
    "\n",
    "# Impute just in case. We'll use these imputed, scaled versions.\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_scaled_imputed = imputer.fit_transform(X_scaled)\n",
    "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1)).ravel()\n",
    "t_imputed = imputer.fit_transform(t.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# --- Step 1: LASSO for outcome model (tw ~ controls) ---\n",
    "# \"What controls predict wealth?\"\n",
    "model_y, selected_y_indices = run_lasso_selection(X_scaled_imputed, y_imputed)\n",
    "selected_y_vars = X.columns[selected_y_indices].tolist()\n",
    "print(f\"Controls selected by outcome model (tw ~ X): {selected_y_vars}\")\n",
    "\n",
    "# --- Step 2: LASSO for treatment model (p401 ~ controls) ---\n",
    "# \"What controls predict participation? (i.e., what causes selection bias?)\"\n",
    "# CONCEPT: Since p401 is binary (0/1), we can't use standard LASSO.\n",
    "# We must use a classifier. LogisticRegression with 'l1' penalty *is* LASSO\n",
    "# for classification.\n",
    "logreg_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('model', LogisticRegression(penalty='l1', solver='liblinear', random_state=42))\n",
    "])\n",
    "# We use GridSearchCV to find the best 'C' (which is 1/alpha, the penalty strength)\n",
    "param_grid = {'model__C': np.logspace(-2, 2, 10)}\n",
    "grid = GridSearchCV(logreg_pipe, param_grid, cv=10, n_jobs=-1)\n",
    "grid.fit(X.values, t_imputed) # Use original X.values for this pipeline\n",
    "\n",
    "logreg_model = grid.best_estimator_.named_steps['model']\n",
    "selected_t_indices = np.where(np.abs(logreg_model.coef_[0]) > 1e-6)[0]\n",
    "selected_t_vars = X.columns[selected_t_indices].tolist()\n",
    "print(f\"Controls selected by treatment model (p401 ~ X): {selected_t_vars}\")\n",
    "\n",
    "# --- Step 3: Union of selected controls ---\n",
    "# This is the \"Double-Selection\" part. We combine both lists.\n",
    "union_vars = list(set(selected_y_vars) | set(selected_t_vars))\n",
    "print(f\"Union of selected controls: {union_vars}\")\n",
    "\n",
    "if not union_vars:\n",
    "    print(\"Warning: DML selected no control variables. Effect may be unreliable.\")\n",
    "    X_final = pd.DataFrame(index=X.index) # Empty dataframe with original index\n",
    "else:\n",
    "    X_final = X[union_vars] # Select only the union of controls\n",
    "    \n",
    "# --- Step 4: Final OLS ---\n",
    "# Now we run a simple OLS (tw ~ p401 + union_of_controls)\n",
    "# This gives the \"debiased\" causal effect of p401.\n",
    "X_ols_dml = pd.concat([t.reset_index(drop=True), X_final.reset_index(drop=True)], axis=1)\n",
    "X_ols_dml.columns = [T_VAR] + union_vars\n",
    "X_ols_dml = sm.add_constant(X_ols_dml, prepend=False) # Add intercept\n",
    "y_ols_dml = y.reset_index(drop=True)\n",
    "\n",
    "dml_model = sm.OLS(y_ols_dml, X_ols_dml).fit()\n",
    "\n",
    "print(dml_model.summary(xname=[T_VAR] + union_vars + ['const']))\n",
    "\n",
    "dml_coef = dml_model.params[T_VAR]\n",
    "dml_se = dml_model.bse[T_VAR]\n",
    "\n",
    "print(\"\\n------------------------------------------------------\")\n",
    "print(\"Project Insight (DML Causal Effect):\")\n",
    "print(f\"The DML estimate for 'p401' is: {dml_coef:.2f} (SE: {dml_se:.2f})\")\n",
    "print(\"This is the 'unbiased' ML-based estimate. Compare this to the\")\n",
    "print(\"Naive LASSO coefficient. This is a more credible causal effect.\")\n",
    "print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Replication of OLS and 2SLS Models\n",
    " \n",
    "This is a required part of your project, replicating the \"traditional\" econometric counterparts.\n",
    "\n",
    "* **OLS (Ordinary Least Squares):** This is the traditional version of Part 1. It will also be **biased** by selection, just like the Naive LASSO.\n",
    "* **2SLS (Two-Stage Least Squares):** This is the traditional *causal* model. It's the ancestor  of the DML model. It \"solves\" selection bias using an **Instrumental Variable (IV)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part 3: Replication of OLS and 2SLS Models ---\n",
      "\n",
      "--- OLS Model (Replication) ---\n",
      "OLS coefficient for 'p401': 12463.10 (SE: 1693.41)\n",
      "This is the standard OLS estimate, which suffers from selection bias.\n",
      "This should be very similar to the Naive LASSO coefficient.\n",
      "\n",
      "--- 2SLS Model (Replication) ---\n",
      "\n",
      "Could not run statsmodels.iv.api.IV2SLS (error: No module named 'statsmodels.iv')\n",
      "Falling back to manual 2SLS for demonstration.\n",
      "\n",
      "--- 2SLS Model (Manual) ---\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     tw   R-squared:                       0.606\n",
      "Model:                            OLS   Adj. R-squared:                  0.605\n",
      "Method:                 Least Squares   F-statistic:                     1085.\n",
      "Date:                Tue, 11 Nov 2025   Prob (F-statistic):               0.00\n",
      "Time:                        19:22:43   Log-Likelihood:            -1.2469e+05\n",
      "No. Observations:                9915   AIC:                         2.494e+05\n",
      "Df Residuals:                    9900   BIC:                         2.495e+05\n",
      "Df Model:                          14                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "p401         1.09e+04   2267.936      4.805      0.000    6451.906    1.53e+04\n",
      "log_inc     1.463e+04   1292.939     11.317      0.000    1.21e+04    1.72e+04\n",
      "age          656.3115     79.038      8.304      0.000     501.381     811.242\n",
      "fsize      -1426.1614    576.467     -2.474      0.013   -2556.153    -296.170\n",
      "marr        8023.3939   2299.835      3.489      0.000    3515.249    1.25e+04\n",
      "twoearn    -1.771e+04   1984.532     -8.922      0.000   -2.16e+04   -1.38e+04\n",
      "db         -2506.3388   1700.198     -1.474      0.140   -5839.073     826.396\n",
      "ira            2.7452      0.097     28.286      0.000       2.555       2.935\n",
      "pira        8091.9131   2218.001      3.648      0.000    3744.180    1.24e+04\n",
      "hown        -1.27e+04   2061.942     -6.158      0.000   -1.67e+04   -8656.309\n",
      "male        3934.5734   1907.152      2.063      0.039     196.167    7672.980\n",
      "educ         476.6601    287.945      1.655      0.098     -87.771    1041.092\n",
      "hmort         -0.2798      0.016    -17.093      0.000      -0.312      -0.248\n",
      "hequity        0.7624      0.013     56.747      0.000       0.736       0.789\n",
      "hval           0.4826      0.010     49.537      0.000       0.463       0.502\n",
      "const      -1.684e+05   1.27e+04    -13.234      0.000   -1.93e+05   -1.43e+05\n",
      "==============================================================================\n",
      "Omnibus:                    15100.656   Durbin-Watson:                   1.972\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         10181894.938\n",
      "Skew:                           9.392   Prob(JB):                         0.00\n",
      "Kurtosis:                     158.863   Cond. No.                     2.55e+15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 2.19e-17. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "*** Warning: Manual 2SLS standard errors are incorrect. ***\n",
      "*** The coefficient estimate, however, is consistent. ***\n",
      "\n",
      "------------------------------------------------------\n",
      "Project Insight (2SLS Causal Effect):\n",
      "The 2SLS (Instrumental Variable) estimate for 'p401' is: 10897.52 (SE: 2267.94)\n",
      "This is the 'traditional' econometric method for finding the\n",
      "causal effect. You should compare this value to the DML (Part 2)\n",
      "estimate. They are two different ways of trying to get the 'true' causal number.\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PART 3: REPLICATION OF OLS AND 2SLS\n",
    "# -----------------------------------\n",
    "print(\"\\n--- Part 3: Replication of OLS and 2SLS Models ---\")\n",
    "\n",
    "# Prepare data for statsmodels (already done as y, t, X, Z)\n",
    "# We must manually add a constant (intercept) for statsmodels\n",
    "X_const = sm.add_constant(X, prepend=False)\n",
    "\n",
    "# --- Model 1: OLS (tw ~ p401 + controls) ---\n",
    "# This is the \"traditional\" econometric counterpart to your naive LASSO\n",
    "X_ols_rep = pd.concat([t, X_const], axis=1)\n",
    "ols_model = sm.OLS(y, X_ols_rep).fit()\n",
    "\n",
    "ols_coef = ols_model.params[T_VAR]\n",
    "ols_se = ols_model.bse[T_VAR]\n",
    "\n",
    "print(\"\\n--- OLS Model (Replication) ---\")\n",
    "print(f\"OLS coefficient for 'p401': {ols_coef:.2f} (SE: {ols_se:.2f})\")\n",
    "print(\"This is the standard OLS estimate, which suffers from selection bias.\")\n",
    "print(\"This should be very similar to the Naive LASSO coefficient.\")\n",
    "\n",
    "# --- Model 2: 2SLS (tw ~ p401 + controls, using e401 as instrument) ---\n",
    "print(\"\\n--- 2SLS Model (Replication) ---\")\n",
    "\n",
    "# Instrumental Variable (IV)\n",
    "# We have selection bias because 'p401' is chosen by people.\n",
    "# An IV ('e401', eligibility) is a variable that:\n",
    "# 1. (Relevance) Strongly predicts participation 'p401'.\n",
    "#    (True: You can't participate if not eligible).\n",
    "# 2. (Exclusion) Affects wealth 'tw' *only through* its effect on 'p401'.\n",
    "#    (Plausible: Being eligible doesn't make you rich *unless* you participate).\n",
    "#\n",
    "# 2SLS uses 'e401' to isolate the \"clean\" part of 'p401' (the part\n",
    "# \"forced\" by eligibility) and uses *only* that to estimate the causal effect.\n",
    "\n",
    "# MOVED: Define IV variables *before* the try/except block\n",
    "# This makes them available to both the 'try' and 'except' scopes.\n",
    "# Note: X_const, T, and Z must have compatible indices\n",
    "Y_iv = y.reset_index(drop=True)\n",
    "X_const_iv = X_const.reset_index(drop=True) # Exogenous controls\n",
    "T_iv = t.reset_index(drop=True)         # Endogenous treatment\n",
    "Z_iv = Z.reset_index(drop=True)         # Instrument\n",
    "\n",
    "try:\n",
    "    # We will try to import and use the \"modern\" IV2SLS module\n",
    "    from statsmodels.iv.api import IV2SLS\n",
    "    \n",
    "    # `dependent` = Y (tw)\n",
    "    # `exog` = All *exogenous* controls (X_const)\n",
    "    # `endog` = The *endogenous* treatment (t = p401)\n",
    "    # `instruments` = The *instrument* (Z = e401)\n",
    "    iv_model = IV2SLS(\n",
    "        dependent=Y_iv,\n",
    "        exog=X_const_iv,\n",
    "        endog=T_iv,\n",
    "        instruments=Z_iv\n",
    "    ).fit()\n",
    "\n",
    "    print(iv_model.summary(xname=X_const.columns.tolist() + [T_VAR]))\n",
    "\n",
    "    iv_coef = iv_model.params[T_VAR]\n",
    "    iv_se = iv_model.bse[T_VAR]\n",
    "        \n",
    "except Exception as e:\n",
    "    # This 'except' block will run if you have an old statsmodels version\n",
    "    print(f\"\\nCould not run statsmodels.iv.api.IV2SLS (error: {e})\")\n",
    "    print(\"Falling back to manual 2SLS for demonstration.\")\n",
    "    \n",
    "    # --- Manual 2SLS ---\n",
    "    # 1st Stage: Regress the endogenous variable (p401) on the\n",
    "    #            instrument (e401) and all controls (X_const).\n",
    "    X_stage1 = pd.concat([Z_iv, X_const_iv], axis=1)\n",
    "    model_stage1 = sm.OLS(T_iv, X_stage1).fit()\n",
    "    p401_predicted = model_stage1.predict(X_stage1)\n",
    "    \n",
    "    # 2nd Stage: Regress the outcome (tw) on the *predicted* p401\n",
    "    #            (p401_predicted) and all controls (X_const).\n",
    "    X_stage2 = pd.concat([p401_predicted, X_const_iv], axis=1)\n",
    "    X_stage2.columns = [T_VAR] + X_const_iv.columns.tolist()\n",
    "    model_stage2 = sm.OLS(Y_iv, X_stage2).fit()\n",
    "\n",
    "    print(\"\\n--- 2SLS Model (Manual) ---\")\n",
    "    print(model_stage2.summary())\n",
    "    iv_coef = model_stage2.params[T_VAR]\n",
    "    iv_se = model_stage2.bse[T_VAR] # NOTE: These SEs are technically incorrect\n",
    "    iv_model = model_stage2 # Save for summary\n",
    "    print(\"\\n*** Warning: Manual 2SLS standard errors are incorrect. ***\")\n",
    "    print(\"*** The coefficient estimate, however, is consistent. ***\")\n",
    "\n",
    "print(\"\\n------------------------------------------------------\")\n",
    "print(\"Project Insight (2SLS Causal Effect):\")\n",
    "print(f\"The 2SLS (Instrumental Variable) estimate for 'p401' is: {iv_coef:.2f} (SE: {iv_se:.2f})\")\n",
    "print(\"This is the 'traditional' econometric method for finding the\")\n",
    "print(\"causal effect. You should compare this value to the DML (Part 2)\")\n",
    "print(\"estimate. They are two different ways of trying to get the 'true' causal number.\")\n",
    "print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Analyzing Heterogeneity\n",
    "\n",
    "This is an optional but important part of your project.\n",
    "\n",
    "**Conceptual Clarification:** \"Heterogeneity\" asks: \"Is the effect of 401(k) participation \n",
    "*the same* for everyone?\"\n",
    "\n",
    "Maybe the effect is *larger* for high-income earners or *smaller* for those with less \n",
    "education. We can test this by adding **interaction terms**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part 4: Analyzing Heterogeneity (Example) ---\n",
      "Testing for heterogeneity by income and education...\n",
      "\n",
      "--- OLS Model with Heterogeneity ---\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     tw   R-squared:                       0.608\n",
      "Model:                            OLS   Adj. R-squared:                  0.608\n",
      "Method:                 Least Squares   F-statistic:                     960.3\n",
      "Date:                Tue, 11 Nov 2025   Prob (F-statistic):               0.00\n",
      "Time:                        19:23:27   Log-Likelihood:            -1.2466e+05\n",
      "No. Observations:                9915   AIC:                         2.493e+05\n",
      "Df Residuals:                    9898   BIC:                         2.495e+05\n",
      "Df Model:                          16                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "p401           -1.548e+05   2.77e+04     -5.595      0.000   -2.09e+05   -1.01e+05\n",
      "p401_x_log_inc  1.628e+04   2757.882      5.904      0.000    1.09e+04    2.17e+04\n",
      "p401_x_educ     -341.9684    624.299     -0.548      0.584   -1565.722     881.785\n",
      "log_inc         1.233e+04   1332.852      9.250      0.000    9716.442    1.49e+04\n",
      "age              645.4572     78.791      8.192      0.000     491.010     799.904\n",
      "fsize          -1443.9992    574.682     -2.513      0.012   -2570.493    -317.506\n",
      "marr            7931.5789   2292.246      3.460      0.001    3438.311    1.24e+04\n",
      "twoearn        -1.841e+04   1980.989     -9.291      0.000   -2.23e+04   -1.45e+04\n",
      "db             -2438.0903   1686.133     -1.446      0.148   -5743.254     867.073\n",
      "ira                2.7033      0.097     27.879      0.000       2.513       2.893\n",
      "pira            7733.6060   2208.813      3.501      0.000    3403.883    1.21e+04\n",
      "hown           -1.178e+04   2060.930     -5.715      0.000   -1.58e+04   -7737.537\n",
      "male            3824.9387   1901.257      2.012      0.044      98.088    7551.790\n",
      "educ             530.1984    323.090      1.641      0.101    -103.123    1163.520\n",
      "hmort             -0.2920      0.016    -17.799      0.000      -0.324      -0.260\n",
      "hequity            0.7662      0.013     57.189      0.000       0.740       0.792\n",
      "hval               0.4739      0.010     48.378      0.000       0.455       0.493\n",
      "const          -1.449e+05   1.31e+04    -11.037      0.000   -1.71e+05   -1.19e+05\n",
      "==============================================================================\n",
      "Omnibus:                    15235.183   Durbin-Watson:                   1.975\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         10737810.547\n",
      "Skew:                           9.550   Prob(JB):                         0.00\n",
      "Kurtosis:                     163.084   Cond. No.                     4.06e+15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 8.62e-18. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "------------------------------------------------------\n",
      "Project Insight (Heterogeneity):\n",
      "To interpret this model:\n",
      " - 'p401' coef: The effect of p401 when log_inc and educ are 0 (not very useful).\n",
      " - 'p401_x_log_inc' coef: How the effect of p401 *changes* with a one-unit increase in log_inc.\n",
      " - 'p401_x_educ' coef: How the effect of p401 *changes* with one more year of education.\n",
      "If the p-value (P>|t|) for an interaction term is significant (e.g., < 0.05),\n",
      "it suggests the effect of 401(k) is *not* one-size-fits-all.\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PART 4: HETEROGENEITY ANALYSIS\n",
    "# ------------------------------\n",
    "print(\"\\n--- Part 4: Analyzing Heterogeneity (Example) ---\")\n",
    "print(\"Testing for heterogeneity by income and education...\")\n",
    "\n",
    "# We will use the main dataset `data`\n",
    "data_het = data.copy()\n",
    "\n",
    "# Create interaction terms\n",
    "# 'p401_x_log_inc' = p401 * log_inc\n",
    "data_het['p401_x_log_inc'] = data_het[T_VAR] * data_het['log_inc']\n",
    "data_het['p401_x_educ'] = data_het[T_VAR] * data_het['educ']\n",
    "\n",
    "Y_het = data_het[Y_VAR]\n",
    "\n",
    "# The new \"treatment\" variables are p401, AND the interaction terms\n",
    "X_het_vars = [T_VAR, 'p401_x_log_inc', 'p401_x_educ'] + CONTROL_VARS\n",
    "X_het = data_het[X_het_vars]\n",
    "X_het = sm.add_constant(X_het, prepend=False) # Add intercept\n",
    "\n",
    "try:\n",
    "    # We run a simple OLS model.\n",
    "    # Note: This OLS model is *also* biased, just like in Part 3.\n",
    "    # A more advanced analysis would interact the *instrument* (2SLS)\n",
    "    # or use a causal forest, but this is a good first step.\n",
    "    model_het = sm.OLS(Y_het, X_het).fit()\n",
    "    print(\"\\n--- OLS Model with Heterogeneity ---\")\n",
    "    print(model_het.summary(\n",
    "        xname=X_het_vars + ['const']\n",
    "    ))\n",
    "    \n",
    "    print(\"\\n------------------------------------------------------\")\n",
    "    print(\"Project Insight (Heterogeneity):\")\n",
    "    print(\"To interpret this model:\")\n",
    "    print(f\" - 'p401' coef: The effect of p401 when log_inc and educ are 0 (not very useful).\")\n",
    "    print(f\" - 'p401_x_log_inc' coef: How the effect of p401 *changes* with a one-unit increase in log_inc.\")\n",
    "    print(f\" - 'p401_x_educ' coef: How the effect of p401 *changes* with one more year of education.\")\n",
    "    print(\"If the p-value (P>|t|) for an interaction term is significant (e.g., < 0.05),\")\n",
    "    print(\"it suggests the effect of 401(k) is *not* one-size-fits-all.\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not run heterogeneity analysis: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY OF 401(k) PARTICIPATION EFFECT ('p401') ON WEALTH ('tw')\n",
      "======================================================================\n",
      "                          Model                        Method  p401_Coefficient  Standard_Error\n",
      "                 1. Naive LASSO        ML Prediction (Biased)           4650.28             NaN\n",
      "             2. Replication OLS          Traditional (Biased)          12463.10         1693.41\n",
      "3. Double-Selection LASSO (DML)          ML Causal (Unbiased)          12463.10         1693.41\n",
      "       4. Replication 2SLS (IV) Traditional Causal (Unbiased)          10897.52         2267.94\n",
      "\n",
      "--- Final Comparison ---\n",
      "This table is the core of your project's comparison.\n",
      "\n",
      "  GROUP 1: BIASED ASSOCIATION (Correlation)\n",
      "   - (1) Naive LASSO & (2) Replication OLS.\n",
      "   - These models show the *association* between p401 and tw.\n",
      "   - They are biased by 'selection' (e.g., savers sign up more).\n",
      "\n",
      "  GROUP 2: CAUSAL EFFECT (Causation)\n",
      "   - (3) DML LASSO & (4) Replication 2SLS.\n",
      "   - These models use different methods to try and *correct* for selection bias.\n",
      "   - (3) uses ML to select the right controls.\n",
      "   - (4) uses an 'instrumental variable' (e401).\n",
      "\n",
      "We should discuss why (1) & (2) differ from (3) & (4).\n"
     ]
    }
   ],
   "source": [
    "# FINAL SUMMARY\n",
    "# ---------------\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY OF 401(k) PARTICIPATION EFFECT ('p401') ON WEALTH ('tw')\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # This table collects the key 'p401' coefficient from all 4 models.\n",
    "    # This is the central table for your project report.\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Model': [\n",
    "            '1. Naive LASSO', \n",
    "            '2. Replication OLS', \n",
    "            '3. Double-Selection LASSO (DML)', \n",
    "            '4. Replication 2SLS (IV)'\n",
    "        ],\n",
    "        'Method': [\n",
    "            'ML Prediction (Biased)', \n",
    "            'Traditional (Biased)', \n",
    "            'ML Causal (Unbiased)', \n",
    "            'Traditional Causal (Unbiased)'\n",
    "        ],\n",
    "        'p401_Coefficient': [\n",
    "            p401_coef_lasso,\n",
    "            ols_model.params[T_VAR],\n",
    "            dml_model.params[T_VAR],\n",
    "            iv_model.params[T_VAR]\n",
    "        ],\n",
    "        'Standard_Error': [\n",
    "            None, # LASSO doesn't provide easy SEs\n",
    "            ols_model.bse[T_VAR],\n",
    "            dml_model.bse[T_VAR],\n",
    "            iv_model.bse[T_VAR] # May be incorrect if manual 2SLS was used\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(summary_df.to_string(index=False, float_format=\"%.2f\"))\n",
    "    \n",
    "    print(\"\\n--- Final Comparison ---\")\n",
    "    print(\"This table is the core of your project's comparison.\")\n",
    "    print(\"\\n  GROUP 1: BIASED ASSOCIATION (Correlation)\")\n",
    "    print(\"   - (1) Naive LASSO & (2) Replication OLS.\")\n",
    "    print(\"   - These models show the *association* between p401 and tw.\")\n",
    "    print(\"   - They are biased by 'selection' (e.g., savers sign up more).\")\n",
    "    \n",
    "    print(\"\\n  GROUP 2: CAUSAL EFFECT (Causation)\")\n",
    "    print(\"   - (3) DML LASSO & (4) Replication 2SLS.\")\n",
    "    print(\"   - These models use different methods to try and *correct* for selection bias.\")\n",
    "    print(\"   - (3) uses ML to select the right controls.\")\n",
    "    print(\"   - (4) uses an 'instrumental variable' (e401).\")\n",
    "    \n",
    "    print(\"\\nWe should discuss why (1) & (2) differ from (3) & (4).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate final summary (models may not have run): {e}\")\n",
    "    print(\"Please review the output of each part above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
