{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "30053450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For not truncating output on large ndarrays.\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "93437752",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('401k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "c1452104",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [\n",
    "  ['<10k', '10-20k', '20-30k', '30-40k', '40-50k', '50-75k', '>75k'],\n",
    "  ['<30', '30-35', '36-44', '45-54', '55+'],\n",
    "  ['<12', '12', '13-16', '16+'],\n",
    "]\n",
    "_ranges = [\n",
    "  [\n",
    "    (0, 9999),\n",
    "    (10000, 19999),\n",
    "    (20000, 29999),\n",
    "    (30000, 39999),\n",
    "    (40000, 49999),\n",
    "    (50000, 74999),\n",
    "    (75000, max(df.inc))\n",
    "  ],\n",
    "  [\n",
    "    (0, 30),\n",
    "    (30, 35),\n",
    "    (36, 44),\n",
    "    (45, 54),\n",
    "    (55, max(df.age))\n",
    "  ],\n",
    "  [\n",
    "    (0, 11),\n",
    "    (12, 12),\n",
    "    (13, 15),\n",
    "    (16, max(df.educ)),\n",
    "  ]\n",
    "]\n",
    "sources = ['inc', 'age', 'educ']\n",
    "\n",
    "def generate_dummies(df: pd.DataFrame):\n",
    "  \"\"\"\n",
    "  Generates dummies for each categorical field described in Chernozhukov et al., 2014.\n",
    "  \"\"\"\n",
    "  for _cats, __ranges, source in zip(cats, _ranges, sources):\n",
    "    for cat, _range in zip(_cats, __ranges):\n",
    "      df[cat] = (\n",
    "        (df[source] >= _range[0]) \n",
    "        & (df[source] <= _range[1])\n",
    "      ).astype(int)\n",
    "\n",
    "generate_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "edaa77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_generate_loginc(df: pd.DataFrame):\n",
    "  \"\"\"\n",
    "  Safe generate loginc. This means that df rows get dropped if income is \n",
    "  <= 0 (a case for a few of them).\n",
    "  \"\"\"\n",
    "  df.drop(\n",
    "    index=df[df.inc <= 0].index,\n",
    "    inplace=True,\n",
    "  )\n",
    "  df['loginc'] = np.log(df.inc)\n",
    "  assert not df.loginc.isnull().all()\n",
    "\n",
    "safe_generate_loginc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "be663138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of controls that does not include p401 (intended for interactions).\n",
    "CONTROLS = ['marr', 'fsize', 'twoearn', 'db', 'pira', 'hown'] + [category for _cats in cats for category in _cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "20c7472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairwise_interactions(df: pd.DataFrame, controls: list[str]) -> tuple[pd.DataFrame, list[str]]:\n",
    "  \"\"\"\n",
    "  Generate pairwise interactions and insert into dataframe 'df' for every field in controls.\n",
    "\n",
    "  Returns: DataFrame with all pairwise interactions, plus the list of combined colnames generated.\n",
    "  \"\"\"\n",
    "  pairs = [\n",
    "    # Generate combinations for all possible pairwise tuples, inclusive of index.\n",
    "    (controls[i], controls[j]) for i in range(0, len(controls)) for j in range(i, len(controls))\n",
    "  ]\n",
    "  combinations_colnames = []\n",
    "  # Using a separate interaction columns list for convenient merging via pd.concat.\n",
    "  interaction_columns = []\n",
    "\n",
    "  for _tuple in pairs:\n",
    "    colname = f'{_tuple[0]}^2' if _tuple[0] == _tuple[1] else f'{_tuple[0]}:{_tuple[1]}'\n",
    "    combinations_colnames.append(colname)\n",
    "    col = (df[_tuple[0]] * df[_tuple[1]]).rename(colname)\n",
    "    interaction_columns.append(col)\n",
    "\n",
    "  return (\n",
    "    pd.concat(\n",
    "      [df, *interaction_columns],\n",
    "      axis=1,\n",
    "    ), \n",
    "    combinations_colnames\n",
    "  )\n",
    "\n",
    "df_with_interactions, combination_colnames = generate_pairwise_interactions(df, CONTROLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "728f4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "def generate_cv_pipeline_and_fit(\n",
    "  X: pd.DataFrame, \n",
    "  y: pd.Series, \n",
    "  reg_type: Literal['lassocv', 'logisticcv'],\n",
    "  cv: int = 10, \n",
    "  random_state: int = 42,\n",
    "  n_jobs: int = -1,\n",
    "  tol: int = 1e-2,\n",
    ") -> list[int]:\n",
    "  \"\"\"\n",
    "  Generates pipeline with requested parameters 'cv', 'random_state', 'n_jobs', and 'tol'. Fits \n",
    "  on requested X and y (pd.DataFrame/Series).\n",
    "\n",
    "  Returns: list of selected covariate indices LassoCV chose in its fitting process.\n",
    "  \"\"\"\n",
    "  if reg_type not in ('lassocv', 'logisticcv'):\n",
    "    raise NotImplementedError(f'Regression type of {reg_type} not defined.')\n",
    "\n",
    "  model_type = 1 if reg_type == 'lassocv' else 2\n",
    "  model = LassoCV(\n",
    "    cv=cv, \n",
    "    random_state=random_state, \n",
    "    n_jobs=n_jobs, \n",
    "    tol=tol, \n",
    "  ) if model_type == 1 else LogisticRegressionCV(\n",
    "    penalty='l1',\n",
    "    cv=cv,\n",
    "    random_state=random_state,\n",
    "    solver='liblinear',\n",
    "    tol=tol\n",
    "  )\n",
    "\n",
    "  steps = ['scaler', 'lassocv' if model_type == 1 else 'logisticcv']\n",
    "  pipeline = Pipeline([\n",
    "    (steps[0], StandardScaler()), \n",
    "    (steps[1], model)\n",
    "  ])\n",
    "\n",
    "  pipeline.fit(X, y)\n",
    "  coefs = pipeline[steps[1]].coef_\n",
    "  selected_indices = np.where(coefs > 1e-6)[0] if model_type == 1 else np.where(coefs[0] > 1e-6)[0]\n",
    "  return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "1af112db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_double_lasso_and_obtain_union_of_variables(\n",
    "  X1: pd.DataFrame, \n",
    "  X2: pd.DataFrame, \n",
    "  y_i: pd.Series, \n",
    "  d_i: pd.Series\n",
    ") -> list[str]:\n",
    "  \"\"\"\n",
    "  Runs double lasso as outlined by Chernozhukov et al. 2014 for the given parameters X1, X2, y_i and d_i. \n",
    "\n",
    "  This is the set of equations:\n",
    "    y_i = \\alpha d_i + x_i'\\theta_y + r_{yi} + \\epsilon_i \n",
    "    d_i = x_i'\\theta_d + r_{di} + v_i\n",
    "\n",
    "  With y_i the outcome of interest and d_i the treatment policy of interest. \\theta_d represents the vector\n",
    "  of coefficients used in prediction of d_i, and \\theta_y represents the vector of coefficients used in prediction\n",
    "  of y_i. We run LASSO on both equations.\n",
    "\n",
    "  Returns: list of variables obtained by the procedure.\n",
    "  \"\"\"\n",
    "  selected_outcome_indices = generate_cv_pipeline_and_fit(X1, y_i, reg_type='lassocv')\n",
    "  selected_outcome_variables = X1.columns[selected_outcome_indices]  \n",
    "  \n",
    "  selected_treatment_indices = generate_cv_pipeline_and_fit(X2, d_i, reg_type='logisticcv')\n",
    "  selected_treatment_variables = X2.columns[selected_treatment_indices]\n",
    "\n",
    "  return list(set(selected_outcome_variables) | set(selected_treatment_variables))\n",
    "\n",
    "X1 = df_with_interactions[CONTROLS + combination_colnames + ['p401']]\n",
    "y_i = df.tw\n",
    "\n",
    "X2 = df_with_interactions[CONTROLS + combination_colnames]\n",
    "d_i = df.p401\n",
    "\n",
    "union_of_selected_vars = run_double_lasso_and_obtain_union_of_variables(X1, X2, y_i, d_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "8c8f9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.reset_option('display.width')\n",
    "pd.reset_option('display.float_format')\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "b2e0b380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...\n",
      "1...\n",
      "2...\n",
      "3...\n",
      "4...\n",
      "5...\n",
      "6...\n",
      "7...\n",
      "8...\n",
      "9...\n",
      "10...\n",
      "11...\n",
      "12...\n",
      "13...\n",
      "14...\n",
      "15...\n",
      "16...\n",
      "17...\n",
      "18...\n",
      "19...\n",
      "20...\n",
      "21...\n",
      "22...\n",
      "23...\n",
      "24...\n",
      "25...\n",
      "26...\n",
      "27...\n",
      "28...\n",
      "29...\n",
      "30...\n",
      "31...\n",
      "32...\n",
      "33...\n",
      "34...\n",
      "35...\n",
      "36...\n",
      "37...\n",
      "38...\n",
      "39...\n",
      "40...\n",
      "41...\n",
      "42...\n",
      "43...\n",
      "44...\n",
      "45...\n",
      "46...\n",
      "47...\n",
      "48...\n",
      "49...\n",
      "50...\n",
      "51...\n",
      "52...\n",
      "53...\n",
      "54...\n",
      "55...\n",
      "56...\n",
      "57...\n",
      "58...\n",
      "59...\n",
      "60...\n",
      "61...\n",
      "62...\n",
      "63...\n",
      "64...\n",
      "65...\n",
      "66...\n",
      "67...\n",
      "68...\n",
      "69...\n",
      "70...\n",
      "71...\n",
      "72...\n",
      "73...\n",
      "74...\n",
      "75...\n",
      "76...\n",
      "77...\n",
      "78...\n",
      "79...\n",
      "80...\n",
      "81...\n",
      "82...\n",
      "83...\n",
      "84...\n",
      "85...\n",
      "86...\n",
      "87...\n",
      "88...\n",
      "89...\n",
      "90...\n",
      "91...\n",
      "92...\n",
      "93...\n",
      "94...\n",
      "95...\n",
      "96...\n",
      "97...\n",
      "98...\n",
      "99...\n",
      "Average alpha: 11713.459820905493\n",
      "Average root mean squared error: 85824.28082773615\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "average_alpha = 0\n",
    "average_rmse = 0\n",
    "X3 = df_with_interactions[union_of_selected_vars]\n",
    "y = df.tw\n",
    "\n",
    "for i in range(0, 100):\n",
    "  print(f'{i}...')\n",
    "\n",
    "  # Generate a 'bootstrapped' alpha coefficient for p401.\n",
    "  X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X3, \n",
    "    y,\n",
    "    test_size=0.2,\n",
    "  )\n",
    "  X_train = sm.add_constant(X_train)\n",
    "  X_test = sm.add_constant(X_test)\n",
    "\n",
    "  # Generate metrics of interest.\n",
    "  res = sm.OLS(y_train, X_train).fit()\n",
    "  y_pred = res.predict(X_test)\n",
    "  rmse = np.sqrt(((y_test - y_pred) ** 2).mean())\n",
    "\n",
    "  average_alpha = (average_alpha * i + res.params['p401']) / (i + 1)\n",
    "  average_rmse = (average_rmse * i + rmse) / (i + 1)\n",
    "\n",
    "print(f'Average alpha: {average_alpha}')\n",
    "print(f'Average root mean squared error: {average_rmse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
